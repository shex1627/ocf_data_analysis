---
title: "OCF Lab Session Analysis Part 1: P(T < s + t | T > t)"
author: "Shicheng Huang"
date: "April 8, 2018"
header-includes:
  - \usepackage{amsmath}
output:
  pdf_document: default
  html_document: default
---
```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = F, cache = T, warning = F, message = F)
```

```{r include=F}
library(dplyr)
library(lubridate)
library(ggplot2)
```

# Introduction
I am a volunteer staff for the Open Computing Facility (OCF) at the University of California, Berkeley, where we 
provide free computer access to all students. Additionally, we also let students print maximum of 10 pages per day and 100 pages per semester.  

As a staff who spends average 7 hours per day in the lab, I often see people waiting for a computer. I wonder if I can have a decent estimate of when people have to wait for a computer. To break down the question, I first try to estimate the wait time for a single computer, since I sometimes have to wait for the particular computer at the corner of the lab. In this post, I will explore how does a desktop session remaining duration distribution changes conditioned on the current session duration. We define **current session duration** the time a student has been on a computer, and the **remaining session duration** how long it will take for him to leave to computer.  

# Session Dataset 

```{r loading_data}
sessions <- read.csv("~/remote/ocf_boc/data/session_duration_public.csv", stringsAsFactors=FALSE, na.strings = "NULL")
staff_sessions <- read.csv("~/remote/ocf_boc/data/staff_session_duration_public.csv", stringsAsFactors=FALSE,na.strings = "NULL")
```

The dataset we use is the lab session data this semester. Below is a snippet of the session data. The field "host" represents each desktop. The field "duration" measure the duration of a session by minutes. 

```{r dataset snippet}
(sessions %>% arrange(desc(end)) %>% head())
```

# Basic Data processing
```{r cleaning}
sessions = sessions %>% 
  filter(complete.cases(sessions)) %>%
  filter(duration > 0) %>%
  filter(host != "blizzard.ocf.berkeley.edu") %>%
  filter(host != "eruption.ocf.berkeley.edu") %>%
  mutate(duration = time_length(interval(start = start, end = end), unit="minute"))

staff_sessions = staff_sessions %>% 
  filter(complete.cases(staff_sessions)) %>%
  mutate(duration = time_length(interval(start = start, end = end), unit="minute"))

public_sessions = sessions %>% anti_join(staff_sessions, by="id")
```

```{r argument_session_data}
sp18 = public_sessions %>% filter(date(start) >= "2018-01-16") %>% arrange(desc(end))
sp18_weekdays = sp18 %>% filter(!(weekdays(date(start)) %in% c("Sunday", "Saturday")))
sp18_staff = staff_sessions %>% filter(date(start) >= '2018-01-16')
wait_times = as.numeric(difftime(sp18$end[1:(length(sp18$end)-1)], sp18$end[2:length(sp18$end)])) 
sp18_wait = sp18 %>% 
  mutate(wait_time = c(0, as.numeric(difftime(sp18$end[1:(length(sp18$end)-1)], sp18$end[2:length(sp18$end)])))) %>%
  mutate(weekday = weekdays(date(start))) %>%
  mutate(hour = hour(end))
```

```{r weekdays, include=F}
# enough evidence to separate between 
sp18 %>% group_by(weekday=weekdays(date(start))) %>% summarise(mean=mean(adjusted_duration),
                                                       median=median(adjusted_duration),
                                                       below5=mean(adjusted_duration<5)
                                                       ) %>% arrange(mean)
```

Here are the procedures I use to clean the data:  

1. Because the lab volunteer staff often uses the desktops much longer than regular users, who mostly come to the lab to print, I exclude all sessions from the volunteer staff.   

2. I filter out sessions that have 0 or negative durations. This is mostly a data engineering issue because it is physically very difficult and rare that some user logins and logouts within 1-2 seconds to have a 0 session duration.  

3. Filter out sessions from host "blizzard.ocf.berkeley.edu" and "eruption.ocf.berkeley" because they are the front desk desktop and desktop specific for volunteer staff to help student organizations with hosting websites.  

# Data Adjustments  

Recall that there used to be a bug in our session tracking infrastructure that we could only record a session's start time at the beginning of the minute (know more about the bug from one of my [previous post)](http://ftdalpha.com/2018/02/20/work-at-ocf-1.html). As a result, lots of session appear to "start" around the beginning of the minute but they actually started the minute before. See below figure about the distribution of session start seconds, the red line is 1/60, the ideal proportion if all session start and end are uniformly random. 

```{r start_end_second}
#par(mfrow=c(1,2))
#hist(second(sp18$start), breaks=60, , prob=T,
#     main="Session Start Seconds",
#     xlab="session start second")
#abline(h=1/60, col="red")

#hist(second(sp18$end), breaks=61, , prob=T,
#     main="Session End Seconds",
#     xlab="session end second", ylim=c(0, 0.035))
#abline(h=1/60, col="red")
```

```{r include=F}
ggplot() +
  geom_bar(aes(x=second(sp18$end))) +
  geom_hline(yintercept =  1/60 * length(second(sp18$start)), color="red") + 
  xlab("session end second")

#The session end seconds also have a peak on 0 and 1, I suspect this is due to a desktop logs out abnormally (from a computer freeze) and some other reasons. But I don't think the small peak of session end seconds would affect the analysis as much since not many sessions are affected.  
```


```{r start_end_seconds}
ggplot() +
  geom_bar(aes(x=second(sp18$start))) +
  geom_hline(yintercept =  1/60 * length(second(sp18$start)), color="red") + 
  xlab("session start second")
```

```{r include=F}
ggplot() +
  geom_bar(aes(x=second(sp18$start[date(sp18$start) <= "2018-02-05"]), 
               group=date(sp18$start[date(sp18$start) <= "2018-02-05"]),
               fill=factor(date(sp18$start[date(sp18$start) <= "2018-02-05"]))), colour="black")
```

```{r include=F}
ggplot() +
  geom_bar(aes(x=second(sp18$start[date(sp18$start) > "2018-02-05"]), 
               group=date(sp18$start[date(sp18$start) > "2018-02-05"]),
               fill=factor(date(sp18$start[date(sp18$start) > "2018-02-05"]))), colour="black") +
  xlab("start second")
# +
#  theme(legend.position="none")
# for some reason 3 is abnormally high, and 0, 1, 2 are low
```

```{r include=F}
post05 = sp18 %>% filter(date(start) > "2018-02-05") 
# starting second 3 has highest count
ggplot(post05 %>% count(hour=hour(start),second=second(start)) %>% filter((hour < 19) & (hour > 8))) +
  geom_line(aes(x=hour, y=n, color=factor(second), group=second)) +
  scale_x_continuous(breaks=0:24,label=0:24)+
  theme(legend.position="none")
# starting second 3 is always the highest

#View(post05 %>% count(hour=hour(start),second=second(start)) %>% tidyr::spread(second, n))
```


To find out the time interval when sessions data that are corrupted, I make a plot shows the percentage of sessions with start seconds < 4 against time. The darker and bigger the dot is, the more sessions are there in the day.    

```{r corrupted_sessions}
corrupted_sessions = data.frame(sp18 %>% 
                    mutate(if_corrupted = second(start) < 4) %>% 
                    group_by(date=date(start)) %>% 
                    summarise(num_corrupted=sum(if_corrupted),
                              num_session=n()) %>%
                    mutate(percent_corrupted=num_corrupted/num_session))
ggplot(corrupted_sessions) +
  geom_point(aes(x=date,y=percent_corrupted, color=-num_session, size=num_session))+ scale_x_date(date_minor_breaks = "1 day") 
```

```{r corrupted_sessions_host?, include=F}
corrupted_sessions = data.frame(sp18 %>% 
                    filter(date(start) > "2018-02-05") %>% 
                    mutate(if_corrupted = second(start) == 3 ) %>% 
                    group_by(date=date(start), host) %>% 
                    summarise(num_corrupted=sum(if_corrupted),
                              num_session=n()) %>%
                    mutate(percent_corrupted=num_corrupted/num_session))
ggplot(corrupted_sessions) +
  geom_point(aes(x=date,y=percent_corrupted, group=host, color=host, size=num_session))+ scale_x_date(date_minor_breaks = "1 day") +
  geom_line(aes(x=date,y=percent_corrupted, group=host, color=host))+ scale_x_date(date_minor_breaks = "1 day")

#, size=num_session
```

We can see the percentage is abnormally high until early February. The "peak" in the end of March is Spring break. As the table below show, after Feb 6th, the session tracking system goes back to normal again. 

```{r corrupted_sessions2}
corrupted_sessions %>% filter(month(date) == 2) %>% head(6) %>% select(date, percent_corrupted, num_session)
```

Thus, for all the session before 2018-02-05, I will adjust the duration by adding a random variable that is uniformly
distributed from the set {0, 1, 2,...55 + $S_{session\,start\,second}$}.   
Because if the session's recorded start time is $X$ given our tracking system was malfunctioning, the real session start
time could be from 0 to $X$ or 5 to 55 from the previous minute. So the real session duration should be anywhere between 0 to $X + 1 + 55$ seconds longer. 

```{r}
set.seed(1234)
sp18 = rbind(
  sp18 %>% filter(date(start) <= "2018-02-05") %>% mutate(adjusted_duration=duration+Vectorize(function(x){sample(0:(x+55),1)/60})(second(start))),
  sp18 %>% filter(date(start) > "2018-02-05") %>% 
mutate(adjusted_duration=duration))

sp18_weekdays = rbind(
  sp18_weekdays %>% filter(date(start) <= "2018-02-05") %>% mutate(adjusted_duration=duration+Vectorize(function(x){sample(0:(x+55),1)/60})(second(start))),
  sp18_weekdays %>% filter(date(start) > "2018-02-05") %>% 
mutate(adjusted_duration=duration))
```

Let's have a rough look at the difference before and after the adjustment through both graph and summary statistics.  
```{r after_adjustment}
y1=as.numeric(table(round(sp18$duration, 1))[1:101])
y2=as.numeric(table(round(sp18$adjusted_duration, 1))[1:101])
x = seq(0, 10, 0.1)
y = rbind(data.frame(count=y1, duration=x,type="original"),
          data.frame(count=y2, duration=x,type="adjusted"))

# summary of all durations
summary(sp18$adjusted_duration)
summary(sp18$duration)

# graph of durations for the first 10 minutes
ggplot(y) +
  geom_line(aes(x=duration, y=count, group=type, color=type)) + 
  scale_x_continuous(breaks=0:10, labels=0:10)
```

There isn't much visible difference but I think it is still important to take good care of the data inaccuracy issue.  

```{r}
#View(sp18 %>% mutate(start_second = second(start)) %>% count(date=date(start), start_second) %>%
#       tidyr::spread(start_second, n))


#View(data.frame(sp18 %>% 
#                    mutate(if_corrupted = second(start) < 10) %>% 
#                    group_by(date=date(start)) %>% 
#                    summarise(num_corrupted=sum(if_corrupted),
#                              num_session=n()) %>%
#                    mutate(percent_corrupted=num_corrupted/num_session)))
```

# Session Analysis

Lets first look at the distribution of the session duration. Because the raw histogram is extremely skewed, I make two other histograms with x axis limit (0, 500) and (0, 50) respectively.  

```{r include=F}
c(length(sp18$adjusted_duration[sp18$adjusted_duration < 500])/length(sp18$adjusted_duration), 
  length(sp18$adjusted_duration[sp18$adjusted_duration < 50])/length(sp18$adjusted_duration),
  length(sp18$adjusted_duration[sp18$adjusted_duration < 10])/length(sp18$adjusted_duration))
```


```{r session_histogram}
par(mfrow=c(1,3))
hist(sp18$adjusted_duration, prob=T, main = "all session duration", xlab="duration in minutes", breaks = 500)
hist(sp18$adjusted_duration, prob=T, main = "duration < 500min (99.9% of data)", xlab="duration in minutes", xlim=c(0, 500), breaks = 500)
hist(sp18$adjusted_duration, prob=T, main = "duration < 50min (925% of data)", xlab="duration in minutes",
     xlim=c(0, 50), breaks = 500)
```

While taking a closer look, it seems like lots of sessions are under 100 minutes. To visualize a skewed distribution, we can also see its different quantiles.  

```{r}
percentiles = seq(0.05, 1, 0.05)
quantiles = round(quantile(sp18$adjusted_duration, percentiles), 1)
```

```{r quantile_graph}
percentiles = seq(0.05, 1, 0.05)
quantiles = round(quantile(sp18$adjusted_duration, percentiles), 1)

ggplot() +
  geom_line(aes(x=percentiles, y=quantiles)) +
  geom_point(aes(x=percentiles, y=quantiles)) +
  geom_text(aes(x=percentiles, y=quantiles, label=quantiles), 
            check_overlap = TRUE,
            vjust = -1) +
  geom_text(aes(x=percentiles, y=quantiles, label=paste0(percentiles,"%")), 
            check_overlap = TRUE,
            vjust = 1) + 
  ylim(-5, 75)
```

We can see 75% of the sessions are under 15 minutes. And 95% of the sessions are under 65 min. Maybe most of the users come to the lab just to print (I will investigate further in part 2). There is a huge jump of session duration from the 95% quantile to the the 100% quantile (722min). To find what are truely outliers, I also make a table showing the quantiles between 95% and 100% with 1% increment. 

```{r}
data.frame(quantiles=round(quantile(sp18$adjusted_duration, seq(0.95, 1, 0.01)), 1))
```

We can see, at least 99% of the sessions are "normal session" which people will leave within 3 hours.  

# Basic Survival  P(additional session duration | session duration & non-staff)

Below graph shows the distributions of remaining session time given the current session duration. If the person has been using the computer for a long time, the distribution of the **remaining session time** will look very red.

```{r}
top=200

par(mfrow=c(1,1))
time_intervals = seq(0, 75, 1)
n_time_intervals = length(time_intervals)
densitys_x = data.frame()
densitys_y = data.frame()
priors = numeric()
```

```{r colors_hex}
#"#ffff00","#fff600",
colors_hex = c(
"#ffec00",
"#ffe300",
"#ffd900",
"#ffd000",
"#ffc600",
"#ffbd00",
"#ffb300",
"#ffaa00",
"#ffa100",
"#ff9700",
"#ff8e00",
"#ff8400",
"#ff7b00",
"#ff7100",
"#ff6800",
"#ff5e00",
"#ff5500",
"#ff4c00",
"#ff4200",
"#ff3900",
"#ff2f00",
"#ff2600",
"#ff1c00",
"#ff1300",
"#ff0000")
```

```{r colors_hex2}
colors_hex_str2 = "#137700
#1c7200
#266d00
#2f6800
#396400
#425f00
#4c5a00
#555500
#5e5100
#684c00
#714700
#7b4200
#843e00
#8e3900
#973400
#a12f00
#aa2b00
#b32600
#bd2100
#c61c00
#d01800
#d91300
#e30e00
#ec0900
#ff0000"

colors_hex2 = strsplit(colors_hex_str2, "\n")[[1]]
```

```{r colors_hex3} 
colors_hex_str3 = "#13ecec
#1ce3e3
#26d9d9
#2fd0d0
#39c6c6
#42bdbd
#4cb3b3
#55aaaa
#5ea1a1
#689797
#718e8e
#7b8484
#847b7b
#8e7171
#976868
#a15e5e
#aa5555
#b34c4c
#bd4242
#c63939
#d02f2f
#d92626
#e31c1c
#ec1313
#ff0000"

colors_hex3 = strsplit(colors_hex_str3, "\n")[[1]]
```

```{r}
#scales = log(1:(n_time_intervals+1)/100)/min(log(1:(n_time_intervals+1)/100))
start = 1
end = 20

plot(density(sp18$adjusted_duration[sp18$adjusted_duration > 0], from=0),
     "Densities", xlim=c(0, 50), col="white")

for(i in start:end) {
  time = time_intervals[i]
  sessions_durations = sp18$adjusted_duration[sp18$adjusted_duration > time]
  temp = density(sessions_durations, from=time)
  #lines(temp$x - time, temp$y, col=rgb(scales[76-time] ,scales[time], 0))
  #lines(temp$x - time, temp$y, col=rgb((i-start)/(end-start), 1-(i-start)/(end-start), 0))
  lines(temp$x - time, temp$y, col=colors_hex3[i])
}
```

```{r}
temp_df = data.frame()
start = 0
end = 25
for (i in start:end) {
  temp_df =rbind(temp_df, c(i, 
                            mean(sp18_weekdays$adjusted_duration[sp18_weekdays$adjusted_duration > i]-i),
                            median(sp18_weekdays$adjusted_duration[sp18_weekdays$adjusted_duration > i]-i)
                            ))
}
colnames(temp_df) = c("t", "mean", "median")
temp_df
```

```{r}
ggplot(temp_df) +
  geom_point(aes(x=t, y=mean, size=(mean-mean(mean))/sd(mean)), colour="red") +
  geom_point(aes(x=t, y=median, size=(median-mean(median))/sd(median)), colour="blue") +
  geom_line(aes(x=t, y=mean), colour="red") +
  geom_line(aes(x=t, y=median), colour="blue") +
  ylab("remaining session duration in minutes") +
  xlab("current session time") +
  theme(legend.position = "none")
```

```{r}
ggplot(temp_df %>% head(4)) +
  geom_point(aes(x=t, y=mean, size=mean), colour="red") +
  geom_line(aes(x=t, y=mean), colour="red") +
  geom_text(aes(x=t, y=mean, label=round(mean,1)), 
            check_overlap = TRUE,
            vjust = -1) + 
  ylab("remaining session duration in minutes") +
  xlab("current session time") +
  ylim(14, 16.5) + 
  theme(legend.position="none")

ggplot(temp_df %>% head(5)) +
  geom_point(aes(x=t, y=median, size=median), colour="blue") +
  geom_line(aes(x=t, y=median), colour="blue") +
  geom_text(aes(x=t, y=median, label=round(median,1)), 
            check_overlap = TRUE,
            vjust = -1) + 
  ylab("remaining session duration in minutes") +
  xlab("current session time") + theme(legend.position="none") + 
  ylim(4, 6.5)
```

since many duration are cluster at the 2 - 3 min mark, we can anticipate a session will last only around 2-3min or so.
As a result, remaining session time (both mean and median) is the lowest when the current session time is 2-3 min. After that, they are monotonically increasing.  

```{r}

plot(density(sp18$adjusted_duration[sp18$adjusted_duration > 0], from=0),
     "Densities", xlim=c(0, 150), col="blue", lwd=2)
time = 2
temp_den = density(sp18$adjusted_duration[sp18$adjusted_duration > time]-time, from=0)
lines(temp_den$x, temp_den$y, type="l", col="red", lwd=2)
time = 3
temp_den = density(sp18$adjusted_duration[sp18$adjusted_duration > time]-time, from=0)
lines(temp_den$x, temp_den$y, type="l", col="green", lwd=2)

time = 5.5
temp_den = density(sp18$adjusted_duration[sp18$adjusted_duration > time]-time, from=0)
lines(temp_den$x, temp_den$y, type="l", col="#5F6372", lwd=2)
time = 13.5
temp_den = density(sp18$adjusted_duration[sp18$adjusted_duration > time]-time, from=0)
lines(temp_den$x, temp_den$y, type="l", col="#8F7F39", lwd=2)
time = 20
temp_den = density(sp18$adjusted_duration[sp18$adjusted_duration > time]-time, from=0)
lines(temp_den$x, temp_den$y, type="l", col="#BF9B00", lwd=2)
```

```{r}
#scales = log(1:(n_time_intervals+1)/100)/min(log(1:(n_time_intervals+1)/100))
start = 1
end = 20

plot(density(sp18$adjusted_duration[sp18$adjusted_duration > 0], from=0),
     "Densities", xlim=c(0, 150), col="white")

for(i in start:end) {
  time = time_intervals[i]
  remaining_sessions_durations = sp18$adjusted_duration[sp18$adjusted_duration > time] - time
  temp = density(remaining_sessions_durations, from=0)
  #lines(temp$x - time, temp$y, col=rgb(scales[76-time] ,scales[time], 0))
  #lines(temp$x - time, temp$y, col=rgb((i-start)/(end-start), 1-(i-start)/(end-start), 0))
  lines(temp$x, temp$y, col=colors_hex3[i])
}
```

Another way to visualize a distribution is to see its different quantiles. The figure below shows the remaining duration of a session colored by the current session duration. 

```{r percentile_quantile_interval_plot}
percentiles = seq(0.05, 1 - 0.05, 0.05)
time_intervals = seq(0, 75, 1)
n_time_intervals = length(time_intervals)

session_quantiles = data.frame()
plot(percentiles, quantile(sp18$adjusted_duration, percentiles), type="l", ylim = c(0, 250), ylab="quantiles (in minutes)")
for(i in 1:length(time_intervals)) {
  time = time_intervals[i]
  sessions_durations = sp18$adjusted_duration[sp18$adjusted_duration > time] - time
  session_quantiles = rbind(session_quantiles, c(quantile(sessions_durations, percentiles), length(sessions_durations)))
  lines(percentiles, quantile(sessions_durations, percentiles), col=colors_hex3[i])
  #col=rgb(time/n_time_intervals, 1-time/n_time_intervals, 0)
  points(percentiles, quantile(sessions_durations, percentiles),col=colors_hex3[i] )
}
colnames(session_quantiles) = c(percentiles, 'n')
```

As the current session duration increases, indicated by the increase "redness" of the line, 
different quantiles of the remaining session time increases.

Below graph looks at how fast the quantiles are increasing as the given session time increases.  

```{r}
plot(time_intervals, session_quantiles$`0.5` ,type = "b", ylim=c(0, 200),
     main="quantiles from 5% to 95%",
     xlab="current(conditional) session duration",
     ylab="quantiles (in minutes)")
n_quantiles = length(percentiles)
for(i in (1:n_quantiles)) {
  #rgb(i/n_quantiles, 1-i/n_quantiles, 0)
  lines(time_intervals, session_quantiles[, i], col=colors_hex3[i])
  points(time_intervals, session_quantiles[, i], col=colors_hex3[i])
}
```

As the current session duration increases, the first quantiles of remaining session duration increases rapidly then slows down after the current session duration exceeds 20. To avoid overlapping lines, I make a graph that only shows the quantiles below 50%.  

```{r}
plot(time_intervals, session_quantiles$`0.5` ,type = "b", ylim=c(0, 35),
     main="quantiles from 5% to 50%",
     xlab="current(conditional) session duration",
     ylab="quantiles (in minutes)")
n_quantiles = length(percentiles)
for(i in (1:as.integer(n_quantiles/2))) {
  lines(time_intervals, session_quantiles[, i], col=colors_hex3[i])
  points(time_intervals, session_quantiles[, i], col=colors_hex3[i])
}
```

From the both graphs, we can learn that, when a user first started his session, there is 50% chance that he will leave within 5-7 minutes (see the left most points when x = 0); however, if a user has been using a desktop for 20 minutes or plus(x > 20), there is 50% chance that he will leave within the next 25 minutes or so, otherwise he will stay probably an additional 25 minutes to 150 minutes.  

Now we can go back to our situation, if the person using the corner computer has been using it for a long time, it is best for your to find another computer or politely ask the person to leave.  

## Future Directions

In part 1, we learn that a majority of the sessions are short but its distribution is extremely right skew with some unreasonable outliers (700+ minutes). Therefore, it is better to look at different quantiles instead. And we find that knowing how long a session has been (i.e the current session time) can help us infer how long the session will last from the current time (i.e remaining session time), but this heuristic's effectiveness will decrease after current session time exceeds 20 minutes or so.

In the next part, I will examine additional variables **day-of-the-week** and **the computer used**. I do expect sessions during the weekends will be longer because it seems like more people come to the lab just to study instead of printing; and students may favor computers differently because of their locations in the lab.  



