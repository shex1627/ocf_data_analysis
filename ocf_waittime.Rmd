---
title: "OCF Lab Session Analysis Part 1: How Long Does It Take To Get a Computer"
author: "Shicheng Huang"
date: "April 8, 2018"
header-includes:
  - \usepackage{amsmath}
output:
  html_document: default
  pdf_document: default
---
```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = F, cache = T, warning = F, message = F)
```

```{r include=F}
library(dplyr)
library(lubridate)
library(ggplot2)

library(grid)
library(gridExtra)
```

# Introduction
I am a volunteer staff for the Open Computing Facility (OCF) at the University of California, Berkeley, where we 
provide free computer access to all students. Additionally, we also let students print maximum of 10 pages per day and 100 pages per semester.  

As a staff who spends average 7 hours per day in the lab, I often see people waiting for a computer. I wonder if I can have a decent estimate of when people have to wait for a computer. To break down the question, I first try to estimate the wait time for a single computer, since I sometimes have to wait for the particular computer at the corner of the lab. In this post, I will explore how does a desktop session remaining duration distribution changes conditioned on the current session duration. We define **current session duration** the time a student has been on a computer, and the **remaining session duration** how long it will take for him to leave to computer.  

# Session Dataset 

```{r loading_data}
sessions <- read.csv("~/remote/ocf_boc/data/session_duration_public.csv", stringsAsFactors=FALSE, na.strings = "NULL")
staff_sessions <- read.csv("~/remote/ocf_boc/data/staff_session_duration_public.csv", stringsAsFactors=FALSE,na.strings = "NULL")
```

The dataset we use is the lab session data this semester. Below is a snippet of the session data. The field "host" represents each desktop. The field "duration" measure the duration of a session by minutes. 

```{r dataset snippet}
(sessions %>% arrange(desc(end)) %>% head())
```

# Basic Data processing
```{r cleaning}
sessions = sessions %>% 
  filter(complete.cases(sessions)) %>%
  filter(duration > 0) %>%
  filter(host != "blizzard.ocf.berkeley.edu") %>%
  filter(host != "eruption.ocf.berkeley.edu") %>%
  mutate(duration = time_length(interval(start = start, end = end), unit="minute"))

staff_sessions = staff_sessions %>% 
  filter(complete.cases(staff_sessions)) %>%
  mutate(duration = time_length(interval(start = start, end = end), unit="minute"))

public_sessions = sessions %>% anti_join(staff_sessions, by="id")
```

```{r argument_session_data}
sp18 = public_sessions %>% filter(date(start) >= "2018-01-16") %>% arrange(desc(end))
sp18_weekdays = sp18 %>% filter(!(weekdays(date(start)) %in% c("Sunday", "Saturday")))
sp18_staff = staff_sessions %>% filter(date(start) >= '2018-01-16')
wait_times = as.numeric(difftime(sp18$end[1:(length(sp18$end)-1)], sp18$end[2:length(sp18$end)])) 
sp18_wait = sp18 %>% 
  mutate(wait_time = c(0, as.numeric(difftime(sp18$end[1:(length(sp18$end)-1)], sp18$end[2:length(sp18$end)])))) %>%
  mutate(weekday = weekdays(date(start))) %>%
  mutate(hour = hour(end))
```



Here are the procedures I use to clean the data:  

1. Because the lab volunteer staff often uses the desktops much longer than regular users, who mostly come to the lab to print, I exclude all sessions from the volunteer staff.   

2. I filter out sessions that have 0 or negative durations. This is mostly a data engineering issue because it is physically very difficult and rare that some user logins and logouts within 1-2 seconds to have a 0 session duration.  

3. Filter out sessions from host "blizzard.ocf.berkeley.edu" and "eruption.ocf.berkeley" because they are the front desk desktop and desktop specific for volunteer staff to help student organizations with hosting websites.  

# Data Adjustments  

Recall that there used to be a bug in our session tracking infrastructure that we could only record a session's start time at the beginning of the minute (know more about the bug from one of my [previous post)](http://ftdalpha.com/2018/02/20/work-at-ocf-1.html). As a result, lots of session appear to "start" around the beginning of the minute but they actually started the minute before. See below figure about the distribution of session start seconds, the red line is 1/60, the ideal proportion if all session start and end are uniformly random. 

```{r start_end_second}
#par(mfrow=c(1,2))
#hist(second(sp18$start), breaks=60, , prob=T,
#     main="Session Start Seconds",
#     xlab="session start second")
#abline(h=1/60, col="red")

#hist(second(sp18$end), breaks=61, , prob=T,
#     main="Session End Seconds",
#     xlab="session end second", ylim=c(0, 0.035))
#abline(h=1/60, col="red")
```

```{r include=F}
ggplot() +
  geom_bar(aes(x=second(sp18$end))) +
  geom_hline(yintercept =  1/60 * length(second(sp18$start)), color="red") + 
  xlab("session end second")

#The session end seconds also have a peak on 0 and 1, I suspect this is due to a desktop logs out abnormally (from a computer freeze) and some other reasons. But I don't think the small peak of session end seconds would affect the analysis as much since not many sessions are affected.  
```


```{r start_end_seconds}
ggplot() +
  geom_bar(aes(x=second(sp18$start))) +
  geom_hline(yintercept =  1/60 * length(second(sp18$start)), color="red") + 
  xlab("session start second")
```

```{r include=F, eval=F}
ggplot() +
  geom_bar(aes(x=second(sp18$start[date(sp18$start) <= "2018-02-05"]), 
               group=date(sp18$start[date(sp18$start) <= "2018-02-05"]),
               fill=factor(date(sp18$start[date(sp18$start) <= "2018-02-05"]))), colour="black")
```

```{r include=F, eval=F}
ggplot() +
  geom_bar(aes(x=second(sp18$start[date(sp18$start) > "2018-02-05"]), 
               group=date(sp18$start[date(sp18$start) > "2018-02-05"]),
               fill=factor(date(sp18$start[date(sp18$start) > "2018-02-05"]))), colour="black") +
  xlab("start second")
# +
#  theme(legend.position="none")
# for some reason 3 is abnormally high, and 0, 1, 2 are low
```

```{r include=F, eval=F}
post05 = sp18 %>% filter(date(start) > "2018-02-05") 
# starting second 3 has highest count
ggplot(post05 %>% count(hour=hour(start),second=second(start)) %>% filter((hour < 19) & (hour > 8))) +
  geom_line(aes(x=hour, y=n, color=factor(second), group=second)) +
  scale_x_continuous(breaks=0:24,label=0:24)+
  theme(legend.position="none")
# starting second 3 is always the highest

#View(post05 %>% count(hour=hour(start),second=second(start)) %>% tidyr::spread(second, n))
```


To find out the time interval when sessions data that are corrupted, I make a plot shows the percentage of sessions with start seconds < 4 against time. The darker and bigger the dot is, the more sessions are there in the day.    

```{r corrupted_sessions_host, include=F}
#corrupted_sessions = data.frame(sp18 %>% 
#                    filter(date(start) > "2018-02-05") %>% 
#                    mutate(if_corrupted = second(start) == 3 ) %>% 
#                    group_by(date=date(start), host) %>% 
#                    summarise(num_corrupted=sum(if_corrupted),
#                             num_session=n()) %>%
#                    mutate(percent_corrupted=num_corrupted/num_session))
#ggplot(corrupted_sessions) +
#  geom_point(aes(x=date,y=percent_corrupted, group=host, color=host, size=num_session))+ scale_x_date(date_minor_breaks = "1 day") +
#  geom_line(aes(x=date,y=percent_corrupted, group=host, color=host))+ scale_x_date(date_minor_breaks = "1 day")

#, size=num_session
```

We can see the percentage is abnormally high until early February. The "peak" in the end of March is Spring break. As the graph below show, after Feb 6th, the session tracking system goes back to normal again. 

```{r corrupted_sessions}
corrupted_sessions = data.frame(sp18 %>% 
                    mutate(if_corrupted = second(start) < 4) %>% 
                    group_by(date=date(start)) %>% 
                    summarise(num_corrupted=sum(if_corrupted),
                              num_session=n()) %>%
                    mutate(percent_corrupted=num_corrupted/num_session))

corrupted_session_plot = ggplot(corrupted_sessions) +
  geom_point(aes(x=date,y=percent_corrupted, color=-num_session, size=num_session)) +
  scale_x_date(date_minor_breaks = "1 day") +
  ylim(0, 1.05) + 
  theme(legend.position = "bottom")

c_session_plot_small = ggplot(corrupted_sessions %>% filter(month(date) == 2) %>% head(10)) +
  geom_point(aes(x=date,y=percent_corrupted, color=-num_session, size=num_session))+ 
  geom_line(aes(x=date,y=percent_corrupted)) + 
  scale_x_date(date_minor_breaks = "1 day") +
  theme(legend.position = "none")

grid.arrange(corrupted_session_plot, c_session_plot_small)
```


Thus, for all the session before 2018-02-05, I will adjust the duration by adding a random variable that is uniformly
distributed from the set {0, 1, 2,...55 + $S_{session\,start\,second}$}.   
Because if the session's recorded start time is $X$ given our tracking system was malfunctioning, the real session start
time could be from 0 to $X$ or 5 to 55 from the previous minute. So the real session duration should be anywhere between 0 to $X + 1 + 55$ seconds longer. 

```{r}
set.seed(1234)
sp18 = rbind(
  sp18 %>% filter(date(start) <= "2018-02-05") %>% mutate(adjusted_duration=duration+Vectorize(function(x){sample(0:(x+55),1)/60})(second(start))),
  sp18 %>% filter(date(start) > "2018-02-05") %>% 
mutate(adjusted_duration=duration))

sp18_weekdays = rbind(
  sp18_weekdays %>% filter(date(start) <= "2018-02-05") %>% mutate(adjusted_duration=duration+Vectorize(function(x){sample(0:(x+55),1)/60})(second(start))),
  sp18_weekdays %>% filter(date(start) > "2018-02-05") %>% 
mutate(adjusted_duration=duration))
```

Let's have a rough look at the difference before and after the adjustment through some summary statistics.  
```{r after_adjustment}
y1=as.numeric(table(round(sp18$duration, 1))[1:101])
y2=as.numeric(table(round(sp18$adjusted_duration, 1))[1:101])
x = seq(0, 10, 0.1)
y = rbind(data.frame(count=y1, duration=x,type="original"),
          data.frame(count=y2, duration=x,type="adjusted"))

# summary of all durations
summary(sp18$adjusted_duration)
summary(sp18$duration)

# graph of durations for the first 10 minutes
#ggplot(y) +
#  geom_line(aes(x=duration, y=count, group=type, color=type)) + 
#  scale_x_continuous(breaks=0:10, labels=0:10)
```

There isn't much visible difference but I think it is still important to take good care of the data inaccuracy issue.  

```{r}
#View(sp18 %>% mutate(start_second = second(start)) %>% count(date=date(start), start_second) %>%
#       tidyr::spread(start_second, n))


#View(data.frame(sp18 %>% 
#                    mutate(if_corrupted = second(start) < 10) %>% 
#                    group_by(date=date(start)) %>% 
#                    summarise(num_corrupted=sum(if_corrupted),
#                              num_session=n()) %>%
#                    mutate(percent_corrupted=num_corrupted/num_session)))
```

```{r weekdays, include=F}
# enough evidence to separate between 
sp18 %>% group_by(weekday=weekdays(date(start))) %>% summarise(mean=mean(adjusted_duration),
                                                       median=median(adjusted_duration),
                                                       below5=mean(adjusted_duration<5)
                                                       ) %>% arrange(mean)
```

# Session Analysis

Lets first look at the distribution of the session duration. Because the raw histogram is extremely skewed, I make two other histograms with x axis limit (0, 500) and (0, 50) respectively.  

```{r include=F}
c(length(sp18$adjusted_duration[sp18$adjusted_duration < 500])/length(sp18$adjusted_duration), 
  length(sp18$adjusted_duration[sp18$adjusted_duration < 50])/length(sp18$adjusted_duration),
  length(sp18$adjusted_duration[sp18$adjusted_duration < 10])/length(sp18$adjusted_duration))
```

While taking a closer look, it seems like lots of sessions are under 100 minutes. To visualize a skewed distribution, we can also see its different quantiles.  

```{r session_histogram}
duration_hist = ggplot()+
  geom_histogram(aes(sp18$adjusted_duration), bins = 500) +
  xlim(0, 500) +
  xlab("duration in minutes")

duration_hist_short = ggplot()+
  geom_histogram(aes(sp18$adjusted_duration), bins = 30, color="gray") +
  xlim(0, 30)+
  xlab("duration in minutes") +
  geom_vline(xintercept=quantile(sp18$adjusted_duration,seq(0.25, 0.75,0.25)), col="red") +
  geom_text(aes(x=quantile(sp18$adjusted_duration,seq(0.25, 0.75,0.25))+2,
                y=c(3999, 3500, 3000), 
                label=c("25%","50%","75%")),color="blue")

grid.arrange(duration_hist, duration_hist_short, ncol=2)
```


```{r quantile_graph, eval=F}
percentiles = seq(0.05, 1, 0.05)
quantiles = round(quantile(sp18$adjusted_duration, percentiles), 1)

ggplot() +
  geom_line(aes(x=percentiles, y=quantiles)) +
  geom_point(aes(x=percentiles, y=quantiles)) +
  geom_text(aes(x=percentiles[c(5, 10, 15)], y=quantiles[c(5, 10, 15)], label=quantiles[c(5, 10, 15)]), 
            check_overlap = TRUE,
            vjust = -1) +
  ylim(-5, 75)
```

We can see 75% of the sessions are under 14 minutes. And 95% of the sessions are under 65 min. In fact, 99% of the sessions are shorter than 145min. Maybe most of the users come to the lab just to print (I will investigate further in part 2).  

# Expected Wait Time Given Current Session Duration

Below graph shows the distributions of remaining session time given the current session duration. If the person has been using the computer for a long time, the distribution of the **remaining session time** will look very red.

```{r}
top=200

par(mfrow=c(1,1))
time_intervals = seq(0, 75, 1)
n_time_intervals = length(time_intervals)
densitys_x = data.frame()
densitys_y = data.frame()
priors = numeric()
```


```{r survival}
temp_df = data.frame()
start = 0
end = 75
for (i in start:end) {
  temp_df =rbind(temp_df, c(i, 
                            mean(sp18_weekdays$adjusted_duration[sp18_weekdays$adjusted_duration > i]-i),
                            median(sp18_weekdays$adjusted_duration[sp18_weekdays$adjusted_duration > i]-i)
                            ))
}
colnames(temp_df) = c("t", "mean_wait", "median_wait")
#temp_df
```

```{r expected_wait_time}
temp_df_melted = temp_df %>% reshape::melt(id=c("t")) %>% rename(metric=variable, wait_time=value)
wait_time_plot = ggplot(temp_df_melted) + 
  geom_point(aes(x=t, y=wait_time, group=metric, color=metric)) +
  geom_line(aes(x=t, y=wait_time, group=metric, color=metric)) + 
  ylab("remaining session duration in minutes") 

```


```{r}
mean_plot = ggplot(temp_df %>% head(4)) +
  geom_point(aes(x=t, y=mean_wait, size=mean_wait), colour="red") +
  geom_line(aes(x=t, y=mean_wait), colour="red") +
  geom_text(aes(x=t, y=mean_wait, label=round(mean_wait,1)), 
            check_overlap = TRUE,
            vjust = -1) + 
  xlab("current session time") +
  ylim(14, 16.5) + 
  theme(legend.position="none")

median_plot = ggplot(temp_df %>% head(5)) +
  geom_point(aes(x=t, y=median_wait, size=median_wait), colour="blue") +
  geom_line(aes(x=t, y=median_wait), colour="blue") +
  geom_text(aes(x=t, y=median_wait, label=round(median_wait,1)), 
            check_overlap = TRUE,
            vjust = -1) +
  xlab("current session time") + theme(legend.position="none") + 
  ylim(4, 6.5)

lay=rbind(c(1,1,2),c(1,1,3))
grid.arrange(wait_time_plot, mean_plot, median_plot, layout_matrix=lay)
``` 


since many duration are cluster at the 2 - 3 min mark, we can anticipate a session will last only around 2-3min or so.
As a result, remaining session time (both mean and median) is the lowest when the current session time is 2-3 min. After that, they are monotonically increasing.  


## Future Directions

In part 1, we learn that a majority of the sessions are short but its distribution is extremely right skew with some unreasonable outliers (700+ minutes). Therefore, it is better to look at different quantiles instead. And we find that knowing how long a session has been (i.e the current session time) can help us infer how long the session will last from the current time (i.e remaining session time), but this heuristic's effectiveness will decrease after current session time exceeds 20 minutes or so.

In the next part, I will examine additional variables **day-of-the-week** and **the computer used**. I do expect sessions during the weekends will be longer because it seems like more people come to the lab just to study instead of printing; and students may favor computers differently because of their locations in the lab.  



